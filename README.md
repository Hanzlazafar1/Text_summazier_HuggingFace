# Text Summarizer using Hugging Face Transformers

![Python](https://img.shields.io/badge/Python-3.x-blue) ![Hugging Face](https://img.shields.io/badge/Hugging%20Face-transformers-yellow) ![License](https://img.shields.io/badge/license-MIT-green)

## Overview
This project is a **Text Summarizer** that uses Hugging Face's Transformer models to summarize long text documents into shorter, meaningful summaries. It leverages the `transformers` library to implement state-of-the-art text summarization with pre-trained models like `T5` and `BART`.

## Features
- Summarizes long paragraphs or documents into concise summaries.
- Supports various Hugging Face models, which can be customized as needed.
- Provides metrics like ROUGE to evaluate summarization quality.

## Table of Contents
- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)
- [Evaluation](#evaluation)
- [Contributing](#contributing)
- [License](#license)

## Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/your-username/text-summarizer.git
   cd text-summarizer
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

   If you haven’t installed `transformers` and `evaluate` libraries:
   ```bash
   pip install transformers evaluate
   ```

## Usage

Here's a quick start guide to use the summarizer in your project:

1. **Load the Model and Tokenizer**:
   ```python
   from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

   # Replace 'model_name' with your choice of summarization model
   model_name = "t5-small"  # or "facebook/bart-large-cnn"
   model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
   tokenizer = AutoTokenizer.from_pretrained(model_name)
   ```

2. **Summarize Text**:
   ```python
   def summarize(text, max_length=130, min_length=30):
       inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
       summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0)
       return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

   text = "Your long text goes here."
   print("Summary:", summarize(text))
   ```

3. **Evaluate Using ROUGE**:
   ```python
   import evaluate

   rouge = evaluate.load("rouge")
   predictions = ["your summarized text"]
   references = ["the reference or original summary"]

   results = rouge.compute(predictions=predictions, references=references)
   print("ROUGE Scores:", results)
   ```

## Examples

Below are example inputs and outputs generated by the summarizer:

**Input:**
```
Machine learning is the study of computer algorithms that improve automatically through experience...
```

**Output:**
```
Machine learning is the study of algorithms that improve through experience.
```

## Evaluation

This project uses the ROUGE metric to evaluate the quality of the generated summaries. ROUGE measures the overlap between the generated summary and the reference summary, providing scores for ROUGE-1, ROUGE-2, and ROUGE-L.

```python
import evaluate

rouge = evaluate.load("rouge")
predictions = ["generated summary"]
references = ["original summary"]

results = rouge.compute(predictions=predictions, references=references)
print("ROUGE Scores:", results)
```

## Contributing

Contributions are welcome! If you’d like to improve the model or add new features, feel free to open a pull request. 

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more information.

---

This README provides a well-organized and professional look, covering installation, usage, evaluation, and contribution guidelines. Customize any specific details to fit your repository's needs. Let me know if you'd like any adjustments!
